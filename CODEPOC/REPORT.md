{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6605b6f-51d0-4df8-9ac8-f2ec72a4e276",
   "metadata": {},
   "source": [
    "# A Deep Reinforcement Learning Approach for Route Planning of Autonomous Vehicles (PoC)\n",
    "\n",
    "**Nom :** HIRECHE Zoulikha  \n",
    "**Simulation :** SUMO + TraCI  \n",
    "**Données :** Réseau routier de Bari (`bari.net.xml`, `bari.sumocfg`)  \n",
    "\n",
    "---\n",
    "\n",
    "## Résumé\n",
    "Ce projet présente un **Proof of Concept (PoC)** basé sur **Paparella et al. (2024)** [1] pour la planification d’itinéraires de véhicules autonomes par **apprentissage par renforcement profond (DRL)** dans **SUMO**.  \n",
    "Le problème est modélisé comme un **processus de décision de Markov (MDP)** (états, actions, récompense) et des politiques **PPO** sont entraînées sur le réseau urbain de Bari.  \n",
    "Nous comparons (i) un **agent global** entraîné sur tout le réseau à (ii) une approche **modulaire** (4 agents entraînés sur 4 zones) combinée via un **manager** (sélection de la meilleure politique par épisode).  \n",
    "L’évaluation repose sur le **taux de succès**, le **retour moyen**, la **longueur d’épisode** et la stabilité des performances.  \n",
    "Les résultats obtenus montrent que l’approche **modulaire + manager** peut améliorer la robustesse et la performance par rapport à l’agent global dans notre configuration PoC et avec un budget d’entraînement limité.\n",
    "\n",
    "> **Note technique (PoC)** : l’article utilise un stack RL différent (souvent Ray/RLlib). Dans ce PoC, nous utilisons **Stable-Baselines3 (PPO)** pour simplifier l’intégration Python/Jupyter et assurer une exécution reproductible. Cette différence peut influencer les temps d’entraînement et certains détails d’implémentation, mais l’objectif de reproduction conceptuelle (MDP, reward, global vs modulaire, manager) reste identique.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction au problème et à sa modélisation\n",
    "La planification d’itinéraires en milieu urbain vise à déterminer, entre un point de départ et une destination, une séquence d’arêtes du réseau routier optimisant plusieurs critères : **temps de parcours**, **confort/sécurité** (éviter des manœuvres brusques ou des trajectoires incohérentes) et **préférences d’infrastructure** (par exemple des voies “prioritaires” pour véhicules autonomes).  \n",
    "Paparella et al. (2024) [1] proposent une approche d’**apprentissage par renforcement profond** entraînée dans un simulateur de trafic (SUMO), et introduisent une stratégie **modulaire** : le réseau est partitionné en zones, un agent est entraîné par zone, puis un **manager** choisit la meilleure solution globale.\n",
    "\n",
    "Dans ce PoC, nous reprenons la structure générale de l’article :\n",
    "- **MDP** (état/action/récompense),\n",
    "- entraînement d’agents **PPO**,\n",
    "- comparaison **global vs modulaire**,\n",
    "- mécanisme de **manager** pour sélectionner la meilleure route.\n",
    "\n",
    "Certaines hypothèses sont simplifiées afin de rendre l’entraînement faisable dans le cadre d’un examen (budget de timesteps limité).\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Critères d’évaluation d’une solution\n",
    "Nous évaluons une politique de planification à l’aide des métriques suivantes :\n",
    "\n",
    "- **Taux de succès (success_rate)** : proportion d’épisodes où le véhicule atteint la destination avant la troncature (limite de pas).  \n",
    "  $$ SR = \\frac{1}{N}\\sum_{i=1}^{N}\\mathbb{1}(\\text{goal\\_reached}_i) $$\n",
    "\n",
    "- **Retour moyen (mean_reward)** : somme des récompenses sur un épisode, moyenne sur plusieurs épisodes.  \n",
    "  $$ \\bar{R} = \\frac{1}{N}\\sum_{i=1}^{N} R_i $$\n",
    "\n",
    "- **Longueur moyenne d’épisode (mean_len)** : nombre moyen de décisions/steps par épisode. Une longueur plus faible indique souvent une convergence plus rapide vers le but dans notre paramétrage.\n",
    "\n",
    "- **Écart-type des retours (std_reward)** : mesure la stabilité/variabilité des performances d’un épisode à l’autre.\n",
    "\n",
    "Ces critères permettent de comparer quantitativement l’agent global, les agents zonés, et la stratégie modulaire/manager.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Jeu de données de référence et métriques\n",
    "\n",
    "### 3.1 Données utilisées\n",
    "Le PoC utilise un réseau routier urbain de Bari converti pour SUMO :\n",
    "- `bari.net.xml` : graphe routier (arêtes, nœuds, vitesses, géométrie),\n",
    "- `bari.sumocfg` : configuration de simulation.\n",
    "\n",
    "Nous filtrons les arêtes conduisibles pour des véhicules “passenger” (arêtes où `allows(\"passenger\")` est vrai).  \n",
    "**Résultat observé** : ~1240 arêtes passenger (selon le réseau fourni).\n",
    "\n",
    "### 3.2 Métriques produites sur le dataset\n",
    "Dans notre exécution :\n",
    "- **Partition en 4 zones** : quadrants SW/SE/NW/NE par médianes des coordonnées (approximation simple et reproductible).\n",
    "- **Priority edges** : approximation des voies AV par les **10% d’arêtes les plus rapides** (top 10% speed).\n",
    "\n",
    "**Hypothèse importante (PoC)** : le dataset OSM/Bari ne fournit pas explicitement des “voies AV”.  \n",
    "Nous modélisons donc l’ensemble des arêtes prioritaires \\(E_p\\) comme les **10% d’arêtes avec la plus grande vitesse**.  \n",
    "Cela approxime l’idée de “priority lanes” de l’article, mais ne correspond pas à une annotation réelle.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Modélisation / résolution + pseudo-code + complexité\n",
    "\n",
    "### 4.1 Modèle MDP (PoC)\n",
    "**État** :  \n",
    "$$ s(t) = [x(t), y(t), \\phi(t), e(t)] $$\n",
    "où \\((x,y)\\) est la position, \\(\\phi\\) l’angle du véhicule, et \\(e\\) l’index (ou identifiant) de l’arête courante.\n",
    "\n",
    "**Actions** :\n",
    "- `0` : conserver la route actuelle  \n",
    "- `1` : `rerouteTraveltime()` (SUMO recalcule un itinéraire estimé via temps de trajet)\n",
    "\n",
    "**Récompense** (forme bornée inspirée de l’article) :\n",
    "- **progrès** : bonus si le temps estimé vers le but diminue, sinon pénalité,  \n",
    "- **pénalité de virage** : pénalité si changement d’angle au-delà d’un seuil,  \n",
    "- **bonus priorité** : bonus si l’arête courante ∈ `priority_edges`,  \n",
    "- **pénalité par step** + **bonus terminal** en cas d’atteinte du but.\n",
    "\n",
    "### 4.2 Stratégie modulaire + manager\n",
    "Nous entraînons 4 agents, chacun limité à une zone. Ensuite, un **manager** exécute des rollouts (ou estimations) et choisit la solution ayant le meilleur retour.\n",
    "\n",
    "**Pseudo-code :**\n",
    "```text\n",
    "Entraîner 1 agent GLOBAL sur tout le réseau.\n",
    "Partitionner le réseau en 4 zones Z1..Z4.\n",
    "Entraîner 4 agents A1..A4, un par zone.\n",
    "\n",
    "Pour évaluer avec MANAGER :\n",
    "  Pour chaque épisode:\n",
    "    exécuter une trajectoire avec chaque agent Ai sur sa zone\n",
    "    obtenir un retour Ri (et succès Si)\n",
    "    choisir i* = argmax_i Ri\n",
    "    la solution MANAGER = trajectoire de Ai*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6d894b-fff0-461a-8028-98af2843a63e",
   "metadata": {},
   "source": [
    "## 4.3 Complexité (estimation)\n",
    "À chaque décision, la simulation peut appeler `findRoute()` et/ou un reroutage.\n",
    "\n",
    "- **Par épisode :**  \n",
    "  $$ O(T \\cdot C_{route}) $$\n",
    "  où \\(T\\) = nombre de décisions (steps) et \\(C_{route}\\) = coût d’un calcul de route.\n",
    "\n",
    "- **En entraînement :**  \n",
    "  $$ O(N \\cdot C_{step}) $$\n",
    "  où \\(N\\) = total de timesteps et \\(C_{step}\\) = coût d’une interaction env (TraCI + reward + éventuel routage).\n",
    "\n",
    "- **Mémoire :** dominée par PPO (réseau MLP) + buffers internes (rollout buffer, normalisation éventuelle, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Scénarios et cas d’usage reproductibles\n",
    "\n",
    "### 5.1 Cas d’usage 1 — Agent global\n",
    "- Entraîner PPO sur toutes les arêtes passenger (`drivable_edges`).  \n",
    "- Tracer la courbe de reward d’entraînement.  \n",
    "- Évaluer sur `n_episodes` (start/goal aléatoires).\n",
    "\n",
    "### 5.2 Cas d’usage 2 — Agents zonés\n",
    "- Construire les zones (`zones[0..3]`).  \n",
    "- Entraîner un agent PPO par zone.  \n",
    "- Tracer 4 courbes (une par agent) de type Fig.5-like.\n",
    "\n",
    "### 5.3 Cas d’usage 3 — Manager\n",
    "- Pour un épisode donné, exécuter **4 rollouts** (un par agent zoné).  \n",
    "- Choisir la solution ayant le **retour maximal**.  \n",
    "- Mesurer la performance moyenne du manager (`mean_reward`, `success_rate`).\n",
    "\n",
    "**Limite (Manager)** : dans ce PoC, chaque agent zoné est évalué dans son **sous-réseau**, ce qui peut impliquer des épisodes (start/goal) **non strictement identiques** entre zones. Les résultats du manager doivent donc être interprétés comme une **approximation** (potentiellement optimiste) de la stratégie de sélection.\n",
    "\n",
    "### 5.4 Instructions de reproduction\n",
    "- Installer SUMO et définir `SUMO_HOME`.  \n",
    "- Placer `bari.net.xml` et `bari.sumocfg` dans le dossier `data/`.  \n",
    "- Exécuter le notebook de code dans l’ordre.  \n",
    "- Les figures et tableaux sont sauvegardés dans `outputs/`.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Expériences numériques et résultats\n",
    "\n",
    "### 6.1 Paramètres d’entraînement (exemple)\n",
    "- Algorithme : **PPO** (Stable-Baselines3).  \n",
    "- Budget : **50k timesteps** (global) et **50k par zone** (modulaire).  \n",
    "- Hyperparamètres principaux :  \n",
    "  - `learning_rate = 2.5e-4`  \n",
    "  - `n_steps = 512`  \n",
    "  - `batch_size = 64`  \n",
    "- Décisions : toutes les `decision_interval` secondes, maximum `max_decisions` par épisode.\n",
    "\n",
    "### 6.2 Résultats (figures et tables)\n",
    "\n",
    "#### Figures\n",
    "- Courbe d’entraînement global : `outputs/global_50k.png`  \n",
    "- Courbes zonées : `outputs/fig5_like_modular_50k.png`\n",
    "\n",
    "Exemple d’insertion d’images dans GitHub :\n",
    "![Global 50k](outputs/global_50k.png)  \n",
    "![Modulaire (4 zones)](outputs/fig5_like_modular_50k.png)\n",
    "\n",
    "#### Tables (issues du notebook)\n",
    "- Évaluation **GLOBAL + ZONES** : `mean_reward`, `std_reward`, `mean_len`, `success_rate`  \n",
    "- Évaluation **MANAGER + résumé** : `mean_reward`, `success_rate`, `zone_counts`  \n",
    "\n",
    "---\n",
    "\n",
    "## 7. Analyse des expériences + limites + reproductibilité\n",
    "\n",
    "### 7.1 Analyse\n",
    "Dans nos résultats (50k timesteps), l’agent **Global** atteint environ :\n",
    "- \\( \\text{mean\\_reward} \\approx -3.38 \\)\n",
    "- \\( \\text{success\\_rate} \\approx 0.43 \\)\n",
    "\n",
    "La stratégie **Modulaire + Manager** atteint :\n",
    "- \\( \\text{mean\\_reward} \\approx +6.04 \\)\n",
    "- \\( \\text{success\\_rate} \\approx 0.87 \\)\n",
    "\n",
    "Ces résultats vont dans le sens de la conclusion de l’article [1] : la décomposition en zones peut réduire la complexité de décision et améliorer la robustesse et/ou la convergence **dans certaines configurations** (dépendantes du réseau, du trafic, et de la reward).\n",
    "\n",
    "### 7.2 Limites et hypothèses\n",
    "- **Priority edges** : approximation par top 10% vitesse (≠ voie AV réelle).  \n",
    "- **Action space** : action simplifiée (`rerouteTraveltime`) ≠ choix explicite gauche/droite/tout droit aux intersections comme dans l’article.  \n",
    "- **Budget d’entraînement** : 50k timesteps (PoC) ≠ entraînement plus long (paper).  \n",
    "- **Trafic / configuration** : la configuration SUMO influence fortement la difficulté des épisodes.  \n",
    "- **Manager** : sélection “best return” dans un cadre PoC (voir remarque section 5.3).\n",
    "- Dans ce PoC, le manager choisit le meilleur retour parmi 4 évaluations faites sur des sous-réseaux; ce n’est pas une hiérarchie exacte comme dans l’article, mais une approximation de la sélection de politique.\n",
    "\n",
    "### 7.3 Retour sur la publication (reproductibilité)\n",
    "L’article fournit la structure générale (MDP, reward, PPO, approche modulaire), mais certains éléments peuvent manquer pour reproduire à l’identique :\n",
    "- paramètres exacts de génération start/goal,  \n",
    "- trafic exact et configuration SUMO,  \n",
    "- détails d’annotation des voies prioritaires,  \n",
    "- seeds et variations de simulation.\n",
    "\n",
    "Notre PoC documente explicitement les hypothèses prises et fournit un pipeline reproductible dans le cadre de l’examen.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Références\n",
    "[1] Paparella et al., 2024 — *A Deep Reinforcement Learning Approach for Route Planning of Autonomous Vehicles*.  \n",
    "[2] Documentation SUMO / TraCI.   \n",
    "[3] OpenStreetMap (données Bari).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c4ab4b-f710-4126-8416-494390830d9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sumo-rl",
   "language": "python",
   "name": "sumo-rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
